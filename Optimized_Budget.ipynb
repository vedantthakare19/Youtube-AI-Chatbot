{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random"
      ],
      "metadata": {
        "id": "03Hl8tQkDsvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)"
      ],
      "metadata": {
        "id": "t9O9FG4JDPN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MarketingEnv:\n",
        "    def __init__(self, betas, budget=100e7):\n",
        "        self.betas = torch.tensor(betas, dtype=torch.float32)  # Revenue coefficients\n",
        "        self.budget = budget  # Total marketing budget\n",
        "        self.num_channels = len(betas)\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Initialize with a random budget allocation.\"\"\"\n",
        "        self.state = np.random.dirichlet(np.ones(self.num_channels)) * self.budget\n",
        "        return torch.tensor(self.state, dtype=torch.float32)\n",
        "\n",
        "    def step(self, action_changes):\n",
        "        \"\"\"Apply action changes and return new state, reward, and done flag.\"\"\"\n",
        "        self.state += action_changes\n",
        "        self.state = np.maximum(self.state, 0)  # Ensure no negative budget\n",
        "\n",
        "        # Normalize to keep the total budget constant\n",
        "        self.state = (self.state / np.sum(self.state)) * self.budget\n",
        "\n",
        "        # Compute revenue (reward)\n",
        "        revenue = np.dot(self.betas.numpy(), self.state)\n",
        "\n",
        "        return torch.tensor(self.state, dtype=torch.float32), revenue, False"
      ],
      "metadata": {
        "id": "Gxmdw8m3BUSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ActorNetwork(nn.Module):\n",
        "    \"\"\"Policy (Actor) Network - Outputs budget allocation probabilities.\"\"\"\n",
        "    def __init__(self, state_dim):\n",
        "        super(ActorNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, 64)\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "        self.fc3 = nn.Linear(64, state_dim)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = torch.relu(self.fc1(state))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return torch.softmax(self.fc3(x), dim=-1)  # Ensure sum = 1"
      ],
      "metadata": {
        "id": "n67xxJSdCFAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CriticNetwork(nn.Module):\n",
        "    \"\"\"Value (Critic) Network - Estimates expected revenue.\"\"\"\n",
        "    def __init__(self, state_dim):\n",
        "        super(CriticNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, 64)\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "        self.fc3 = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = torch.relu(self.fc1(state))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.fc3(x)  # Single value output"
      ],
      "metadata": {
        "id": "PUw-NVdOCLCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === PPO AGENT ===\n",
        "class PPOAgent:\n",
        "    def __init__(self, state_dim, lr=0.002, gamma=0.99, epsilon=0.2):\n",
        "        self.actor = ActorNetwork(state_dim)\n",
        "        self.critic = CriticNetwork(state_dim)\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr)\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr)\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def compute_advantage(self, rewards, values):\n",
        "        \"\"\"Calculate advantage function (TD error).\"\"\"\n",
        "        advantages = []\n",
        "        returns = []\n",
        "        G = 0\n",
        "        for r, v in zip(reversed(rewards), reversed(values)):\n",
        "            G = r + self.gamma * G\n",
        "            returns.insert(0, G)\n",
        "            advantages.insert(0, G - v)\n",
        "        return torch.tensor(advantages, dtype=torch.float32), torch.tensor(returns, dtype=torch.float32)\n",
        "\n",
        "    def update(self, states, actions, rewards, old_probs):\n",
        "        \"\"\"Perform PPO update.\"\"\"\n",
        "        values = self.critic(states).squeeze()\n",
        "        advantages, returns = self.compute_advantage(rewards, values)\n",
        "\n",
        "        # Compute new probabilities\n",
        "        new_probs = self.actor(states).gather(1, actions.unsqueeze(1)).squeeze()\n",
        "        ratio = new_probs / old_probs\n",
        "\n",
        "        # PPO clipped objective\n",
        "        surrogate1 = ratio * advantages\n",
        "        surrogate2 = torch.clamp(ratio, 1 - self.epsilon, 1 + self.epsilon) * advantages\n",
        "        actor_loss = -torch.min(surrogate1, surrogate2).mean()\n",
        "\n",
        "        # Value loss (Critic update)\n",
        "        critic_loss = (returns - values).pow(2).mean()\n",
        "\n",
        "        # Backpropagation\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        self.critic_optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        self.critic_optimizer.step()"
      ],
      "metadata": {
        "id": "JM1G2GnCCW2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === TRAINING PPO AGENT ===\n",
        "betas = [2, 3, 1.5, 2.5, 3.5, 2, 1, 1.8, 2.2]\n",
        "\n",
        "env = MarketingEnv(betas)\n",
        "ppo_agent = PPOAgent(state_dim=len(betas))\n",
        "\n",
        "num_episodes = 2000\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    states, actions, rewards, old_probs = [], [], [], []\n",
        "\n",
        "    for t in range(10):\n",
        "        state_tensor = state.clone().detach() if isinstance(state, torch.Tensor) else torch.tensor(state, dtype=torch.float32)\n",
        "        action_probs = ppo_agent.actor(state_tensor)\n",
        "        action = torch.multinomial(action_probs, 1).item()\n",
        "\n",
        "        action_changes = np.zeros(len(betas))\n",
        "        action_changes[action] = np.random.uniform(-5, 5)\n",
        "\n",
        "        next_state, reward, done = env.step(action_changes)\n",
        "\n",
        "        states.append(state.numpy())  # Convert tensor to numpy before storing\n",
        "        actions.append(action)\n",
        "        rewards.append(reward)\n",
        "        old_probs.append(action_probs[action].item())\n",
        "\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "    # Convert lists to tensors for training\n",
        "    states_tensor = torch.tensor(np.array(states), dtype=torch.float32)\n",
        "    actions_tensor = torch.tensor(actions, dtype=torch.long)\n",
        "    rewards_tensor = torch.tensor(rewards, dtype=torch.float32)\n",
        "    old_probs_tensor = torch.tensor(old_probs, dtype=torch.float32)\n",
        "\n",
        "    ppo_agent.update(states_tensor, actions_tensor, rewards_tensor, old_probs_tensor)\n",
        "\n",
        "    if episode % 100 == 0:\n",
        "        print(f\"Episode {episode}, Total Revenue: {total_reward:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "816KXQf8DlXG",
        "outputId": "c1f59a39-5a7d-4d7b-9d90-e6bc1ea81926"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0, Total Revenue: 21389237089.75\n",
            "Episode 100, Total Revenue: 20282050517.59\n",
            "Episode 200, Total Revenue: 20533749784.23\n",
            "Episode 300, Total Revenue: 21161017971.12\n",
            "Episode 400, Total Revenue: 20238844801.21\n",
            "Episode 500, Total Revenue: 22486703062.88\n",
            "Episode 600, Total Revenue: 20672598555.15\n",
            "Episode 700, Total Revenue: 20214069393.61\n",
            "Episode 800, Total Revenue: 19242210640.08\n",
            "Episode 900, Total Revenue: 21429024595.86\n",
            "Episode 1000, Total Revenue: 20167532759.17\n",
            "Episode 1100, Total Revenue: 17527061239.01\n",
            "Episode 1200, Total Revenue: 22339779943.21\n",
            "Episode 1300, Total Revenue: 23384486569.84\n",
            "Episode 1400, Total Revenue: 18280377603.63\n",
            "Episode 1500, Total Revenue: 21631241027.26\n",
            "Episode 1600, Total Revenue: 16807202393.97\n",
            "Episode 1700, Total Revenue: 19808364055.26\n",
            "Episode 1800, Total Revenue: 23557193894.79\n",
            "Episode 1900, Total Revenue: 19424840827.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === DISPLAY FINAL OPTIMIZED BUDGET ===\n",
        "final_budget = env.state\n",
        "print(\"\\nOptimized Marketing Budget Allocation:\")\n",
        "for i, category in enumerate([\"TV\", \"Digital\", \"Sponsorship\", \"Content\", \"Online\", \"Affiliates\", \"SEM\", \"Radio\", \"Others\"]):\n",
        "    print(f\"{category}: ${final_budget[i]:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzaRRkReDpGB",
        "outputId": "d3eb7aa3-ae10-4a36-8055-057d6504ed57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Optimized Marketing Budget Allocation:\n",
            "TV: $20643418.09\n",
            "Digital: $32660597.96\n",
            "Sponsorship: $33849653.40\n",
            "Content: $108285599.16\n",
            "Online: $313263997.77\n",
            "Affiliates: $6193711.69\n",
            "SEM: $122030557.95\n",
            "Radio: $64716400.71\n",
            "Others: $298356063.26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Given betas (fixed revenue multipliers)\n",
        "betas = np.array([2, 3, 1.5, 2.5, 3.5, 2, 1, 1.8, 2.2])\n",
        "\n",
        "# Initial budget allocation (before training, assume uniform allocation)\n",
        "initial_budget = np.full(len(betas), 100e7 / len(betas))  # Evenly distribute total budget 100\n",
        "\n",
        "# Optimized budget allocation (after PPO training)\n",
        "optimized_budget = env.state # PPO gives final budget allocation\n",
        "\n",
        "# Compute revenues\n",
        "initial_revenue = np.dot(betas, initial_budget)\n",
        "optimized_revenue = np.dot(betas, optimized_budget)\n",
        "\n",
        "# Total budget (set it based on your requirement, e.g., 100 or another value)\n",
        "total_budget = np.sum(initial_budget)  # Ensuring budget consistency\n",
        "\n",
        "# Compute ROI\n",
        "initial_ROI = ((initial_revenue - total_budget) / total_budget) * 100\n",
        "optimized_ROI = ((optimized_revenue - total_budget) / total_budget) * 100\n",
        "\n",
        "# ROI Increase\n",
        "roi_increase = optimized_ROI - initial_ROI\n",
        "\n",
        "# Print Results\n",
        "print(f\"Initial ROI: {initial_ROI:.2f}%\")\n",
        "print(f\"Optimized ROI: {optimized_ROI:.2f}%\")\n",
        "print(f\"Increase in ROI: {roi_increase:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUlaF2UeMO5U",
        "outputId": "ae470a57-9b58-4704-f5c7-b163437b1091"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial ROI: 116.67%\n",
            "Optimized ROI: 146.45%\n",
            "Increase in ROI: 29.78%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BkV_F431MPuD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}